{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69936968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39fb5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# PID controller\n",
    "# -----------------------\n",
    "class PIDController:\n",
    "    def __init__(self, Kp=0.6, Ki=0.01, Kd=0.1):\n",
    "        self.Kp = Kp\n",
    "        self.Ki = Ki\n",
    "        self.Kd = Kd\n",
    "        self.prev_error = 0.0\n",
    "        self.integral = 0.0\n",
    "        self.prev_time = None\n",
    "\n",
    "    def step(self, error):\n",
    "        now = time.time()\n",
    "        dt = (now - self.prev_time) if self.prev_time else 0.033\n",
    "        self.prev_time = now\n",
    "        dt = max(1e-3, dt)\n",
    "\n",
    "        self.integral += error * dt\n",
    "        derivative = (error - self.prev_error) / dt\n",
    "        output = self.Kp * error + self.Ki * self.integral + self.Kd * derivative\n",
    "        self.prev_error = error\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bbc036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Lane detection\n",
    "# -----------------------\n",
    "def detect_and_annotate(frame, mtx=None, dist=None):\n",
    "    if mtx is not None and dist is not None:\n",
    "        frame = cv2.undistort(frame, mtx, dist, None, mtx)\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blur, 50, 150)\n",
    "\n",
    "    mask = np.zeros_like(edges)\n",
    "    polygon = np.array([[(0,h),(w,h),(w,int(h*0.6)),(0,int(h*0.6))]], np.int32)\n",
    "    cv2.fillPoly(mask, polygon, 255)\n",
    "    roi = cv2.bitwise_and(edges, mask)\n",
    "\n",
    "    lines = cv2.HoughLinesP(roi, 1, np.pi/180, 50, minLineLength=40, maxLineGap=120)\n",
    "\n",
    "    left_points, right_points = [], []\n",
    "    if lines is not None:\n",
    "        for l in lines:\n",
    "            x1,y1,x2,y2 = l[0]\n",
    "            if x2 == x1: continue\n",
    "            slope = (y2-y1)/(x2-x1)\n",
    "            if slope < -0.3: left_points += [(x1,y1),(x2,y2)]\n",
    "            elif slope > 0.3: right_points += [(x1,y1),(x2,y2)]\n",
    "\n",
    "    def fit_line(points):\n",
    "        if len(points) < 2: return None\n",
    "        ys = np.array([p[1] for p in points])\n",
    "        xs = np.array([p[0] for p in points])\n",
    "        a,b = np.polyfit(ys,xs,1)\n",
    "        y1,y2 = h,int(h*0.6)\n",
    "        x1,x2 = int(a*y1+b), int(a*y2+b)\n",
    "        return (x1,y1,x2,y2)\n",
    "\n",
    "    left_line, right_line = fit_line(left_points), fit_line(right_points)\n",
    "\n",
    "    overlay = np.zeros_like(frame)\n",
    "    if left_line is not None:\n",
    "        cv2.line(overlay, (left_line[0], left_line[1]), (left_line[2], left_line[3]), (255,0,0), 10)\n",
    "    if right_line is not None:\n",
    "        cv2.line(overlay, (right_line[0], right_line[1]), (right_line[2], right_line[3]), (0,0,255), 10)\n",
    "\n",
    "    annotated = cv2.addWeighted(frame,0.8,overlay,1.0,0.0)\n",
    "\n",
    "    if left_line and right_line:\n",
    "        left_x = (left_line[0]+left_line[2])/2.0\n",
    "        right_x = (right_line[0]+right_line[2])/2.0\n",
    "        lane_center = (left_x+right_x)/2.0\n",
    "        image_center = w/2.0\n",
    "        offset = (lane_center-image_center)/(w/2.0)\n",
    "    else:\n",
    "        offset = 0.0\n",
    "\n",
    "    cv2.line(annotated, (w//2,h), (w//2,int(h*0.6)), (0,255,0),2)\n",
    "    return offset, annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dc3399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Traffic light color classification (HSV)\n",
    "# -----------------------\n",
    "def classify_traffic_light(crop):\n",
    "    hsv = cv2.cvtColor(crop, cv2.COLOR_BGR2HSV)\n",
    "    masks = {\n",
    "        \"Red\": cv2.inRange(hsv,(0,100,100),(10,255,255)) | cv2.inRange(hsv,(160,100,100),(179,255,255)),\n",
    "        \"Yellow\": cv2.inRange(hsv,(15,100,100),(35,255,255)),\n",
    "        \"Green\": cv2.inRange(hsv,(40,100,100),(90,255,255))\n",
    "    }\n",
    "    counts = {k: cv2.countNonZero(v) for k,v in masks.items()}\n",
    "    return max(counts, key=counts.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d587ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Camera calibration loader\n",
    "# -----------------------\n",
    "def load_calibration(pickle_path='calibration_pickle.p'):\n",
    "    if os.path.exists(pickle_path):\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data.get('mtx'), data.get('dist')\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dd7b67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded YOLO classes: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "Loaded Speed classes: {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3', 4: 'class_4', 5: 'class_5', 6: 'class_6', 7: 'class_7', 8: 'class_8', 9: 'class_9', 10: 'class_14', 11: 'class_22', 12: 'class_26', 13: 'class_27', 14: 'class_28', 15: 'class_29'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 88\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Run pipeline\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 0 = webcam, or provide video path\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 28\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(input_source, output_path)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded Speed classes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, speed_model\u001b[38;5;241m.\u001b[39mnames)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Main video processing pipeline\n",
    "# -----------------------\n",
    "def process_video(input_source=0, output_path=None):\n",
    "    mtx, dist = load_calibration()\n",
    "    cap = cv2.VideoCapture(input_source)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Cannot open video\")\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    out = None\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
    "\n",
    "    pid = PIDController()\n",
    "\n",
    "    # Load YOLO models\n",
    "    model = YOLO(r\"D:\\lane_detection_handover\\yolov8n.pt\")          # Main YOLO (traffic lights + objects)\n",
    "    speed_model = YOLO(r\"D:\\lane_detection_handover\\my_traffic_model.pt\")  # Speed limit detection\n",
    "    print(\"Loaded YOLO classes:\", model.names)\n",
    "    print(\"Loaded Speed classes:\", speed_model.names)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 1️⃣ Lane detection\n",
    "        offset, lane_annotated = detect_and_annotate(frame, mtx, dist)\n",
    "        steer = pid.step(-offset)\n",
    "        annotated = lane_annotated.copy()\n",
    "\n",
    "        # 2️⃣ Main YOLO detection\n",
    "        results = model(frame, conf=0.3, device=\"cpu\", verbose=False)\n",
    "        for r in results:\n",
    "            for box in r.boxes:\n",
    "                cls_id = int(box.cls[0])\n",
    "                label = model.names[cls_id]\n",
    "                conf = float(box.conf[0])\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "                if label.lower() == \"traffic light\":\n",
    "                    crop = frame[y1:y2, x1:x2]\n",
    "                    if crop.size > 0:\n",
    "                        color = classify_traffic_light(crop)\n",
    "                        cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "                        cv2.putText(annotated, f\"{color}\", (x1, y1 - 10),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "                else:\n",
    "                    cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.putText(annotated, f\"{label} {conf:.2f}\", (x1, y1 - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # 3️⃣ Speed detection YOLO\n",
    "        speed_results = speed_model(frame, conf=0.3, device=\"cpu\", verbose=False)\n",
    "        for r in speed_results:\n",
    "            for box in r.boxes:\n",
    "                cls_id = int(box.cls[0])\n",
    "                label = speed_model.names[cls_id]\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                cv2.rectangle(annotated, (x1, y1), (x2, y2), (255, 0, 255), 2)\n",
    "                cv2.putText(annotated, f\"Speed:{label}\", (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 255), 2)\n",
    "\n",
    "        # 4️⃣ Steering overlay\n",
    "        cv2.putText(annotated, f\"Steer:{steer:+.2f}\", (20, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Save/show frame\n",
    "        if out:\n",
    "            out.write(annotated)\n",
    "        cv2.imshow(\"Full Pipeline\", annotated)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    if out: out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# -----------------------\n",
    "# Run pipeline\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    process_video(0)  # 0 = webcam, or provide video path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b3010e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (testingopenai)",
   "language": "python",
   "name": "testingopenai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
